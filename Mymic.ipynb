{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets accelerate gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, AutoModelForCausalLM, DataCollatorForLanguageModeling, pipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "db_path = \"/content/drive/My Drive/chat_backup.db\"\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "first_chat = 'FIRST NUMBER'\n",
    "\n",
    "first_query = f\"\"\"\n",
    "SELECT\n",
    "  datetime(m.date / 1000000000 + strftime('%s', '2001-01-01'), 'unixepoch') AS timestamp,\n",
    "  m.is_from_me,\n",
    "  COALESCE(h.id, 'You') AS sender,\n",
    "  m.text\n",
    "FROM\n",
    "  message m\n",
    "LEFT JOIN\n",
    "  handle h ON m.handle_id = h.rowid\n",
    "JOIN\n",
    "  chat_message_join cmj ON m.rowid = cmj.message_id\n",
    "JOIN\n",
    "  chat c ON cmj.chat_id = c.rowid\n",
    "WHERE\n",
    "  c.chat_identifier = '{first_chat}'\n",
    "  AND m.text IS NOT NULL\n",
    "ORDER BY\n",
    "  timestamp ASC;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df1 = pd.read_sql_query(first_query, conn)\n",
    "\n",
    "second_chat = 'SECOND NUMBER'\n",
    "\n",
    "second_query = f\"\"\"\n",
    "SELECT\n",
    "  datetime(m.date / 1000000000 + strftime('%s', '2001-01-01'), 'unixepoch') AS timestamp,\n",
    "  m.is_from_me,\n",
    "  COALESCE(h.id, 'You') AS sender,\n",
    "  m.text\n",
    "FROM\n",
    "  message m\n",
    "LEFT JOIN\n",
    "  handle h ON m.handle_id = h.rowid\n",
    "JOIN\n",
    "  chat_message_join cmj ON m.rowid = cmj.message_id\n",
    "JOIN\n",
    "  chat c ON cmj.chat_id = c.rowid\n",
    "WHERE\n",
    "  c.chat_identifier = '{second_chat}'\n",
    "  AND m.text IS NOT NULL\n",
    "ORDER BY\n",
    "  timestamp ASC;\n",
    "\"\"\"\n",
    "\n",
    "df2 = pd.read_sql_query(second_query, conn)\n",
    "\n",
    "third_chat = 'THIRD NUMBER'\n",
    "\n",
    "third_query = f\"\"\"\n",
    "SELECT\n",
    "  datetime(m.date / 1000000000 + strftime('%s', '2001-01-01'), 'unixepoch') AS timestamp,\n",
    "  m.is_from_me,\n",
    "  COALESCE(h.id, 'You') AS sender,\n",
    "  m.text\n",
    "FROM\n",
    "  message m\n",
    "LEFT JOIN\n",
    "  handle h ON m.handle_id = h.rowid\n",
    "JOIN\n",
    "  chat_message_join cmj ON m.rowid = cmj.message_id\n",
    "JOIN\n",
    "  chat c ON cmj.chat_id = c.rowid\n",
    "WHERE\n",
    "  c.chat_identifier = '{third_chat}'\n",
    "  AND m.text IS NOT NULL\n",
    "ORDER BY\n",
    "  timestamp ASC;\n",
    "\"\"\"\n",
    "\n",
    "df3 = pd.read_sql_query(third_query, conn)\n",
    "\n",
    "fourth_chat = 'FOURTH NUMBER'\n",
    "\n",
    "fourth_query = f\"\"\"\n",
    "SELECT\n",
    "  datetime(m.date / 1000000000 + strftime('%s', '2001-01-01'), 'unixepoch') AS timestamp,\n",
    "  m.is_from_me,\n",
    "  COALESCE(h.id, 'You') AS sender,\n",
    "  m.text\n",
    "FROM\n",
    "  message m\n",
    "LEFT JOIN\n",
    "  handle h ON m.handle_id = h.rowid\n",
    "JOIN\n",
    "  chat_message_join cmj ON m.rowid = cmj.message_id\n",
    "JOIN\n",
    "  chat c ON cmj.chat_id = c.rowid\n",
    "WHERE\n",
    "  c.chat_identifier = '{fourth_chat}'\n",
    "  AND m.text IS NOT NULL\n",
    "ORDER BY\n",
    "  timestamp ASC;\n",
    "\"\"\"\n",
    "\n",
    "df4 = pd.read_sql_query(fourth_query, conn)\n",
    "\n",
    "fifth_chat = 'FIFTH NUMBER'\n",
    "\n",
    "fifth_query = f\"\"\"\n",
    "SELECT\n",
    "  datetime(m.date / 1000000000 + strftime('%s', '2001-01-01'), 'unixepoch') AS timestamp,\n",
    "  m.is_from_me,\n",
    "  COALESCE(h.id, 'You') AS sender,\n",
    "  m.text\n",
    "FROM\n",
    "  message m\n",
    "LEFT JOIN\n",
    "  handle h ON m.handle_id = h.rowid\n",
    "JOIN\n",
    "  chat_message_join cmj ON m.rowid = cmj.message_id\n",
    "JOIN\n",
    "  chat c ON cmj.chat_id = c.rowid\n",
    "WHERE\n",
    "  c.chat_identifier = '{fifth_chat}'\n",
    "  AND m.text IS NOT NULL\n",
    "ORDER BY\n",
    "  timestamp ASC;\n",
    "\"\"\"\n",
    "\n",
    "df5 = pd.read_sql_query(fifth_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(df):\n",
    "    messages = df.to_dict(\"records\")\n",
    "    conversations = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(messages) - 1:\n",
    "        msg = messages[i]\n",
    "\n",
    "        # Only start a pair when the message is not me\n",
    "        if msg[\"is_from_me\"] == 0:\n",
    "            incoming = [msg[\"text\"]]\n",
    "            i += 1\n",
    "\n",
    "            # Collect multiple messages (before i reply)\n",
    "            while i < len(messages) and messages[i][\"is_from_me\"] == 0:\n",
    "                incoming.append(messages[i][\"text\"])\n",
    "                i += 1\n",
    "\n",
    "            # Now collect my reply/replies\n",
    "            outgoing = []\n",
    "            while i < len(messages) and messages[i][\"is_from_me\"] == 1:\n",
    "                outgoing.append(messages[i][\"text\"])\n",
    "                if i + 1 < len(messages) and messages[i + 1][\"is_from_me\"] == 0:\n",
    "                    break\n",
    "                i += 1\n",
    "\n",
    "            # Save the pair if both sides have content\n",
    "            if incoming and outgoing:\n",
    "                prompt = \"Friend: \" + \"\\n\".join(incoming) + \"\\nYou:\"\n",
    "                completion = \" \" + \"\\n\".join(outgoing)\n",
    "                conversations.append({\n",
    "                    \"prompt\": prompt.strip(),\n",
    "                    \"completion\": completion.strip()\n",
    "                })\n",
    "        else:\n",
    "            i += 1\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo1 = parser(df1)\n",
    "convo2 = parser(df2)\n",
    "convo3 = parser(df3)\n",
    "convo4 = parser(df4)\n",
    "convo5 = parser(df5)\n",
    "all_convos = convo1 + convo2 + convo3 + convo4 + convo5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(all_convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now split into train/test\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(example):\n",
    "  text = example[\"prompt\"] + example[\"completion\"]\n",
    "  return tokenizer(text, padding=True, truncation=True, max_length=1024)\n",
    "\n",
    "tokenized = split_dataset.map(tokenize, batched=False)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Flatten and ignore -100s\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        torch.tensor(logits).view(-1, logits.shape[-1]),\n",
    "        torch.tensor(labels).view(-1),\n",
    "        ignore_index=-100,\n",
    "        reduction='mean'\n",
    "    )\n",
    "    return {\"perplexity\": math.exp(loss.item())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./gpt2-finetuned\",\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    num_train_epochs = 3,\n",
    "    logging_steps = 100,\n",
    "    save_total_limit = 2,\n",
    "    fp16 = True,\n",
    "    push_to_hub = False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized[\"train\"],\n",
    "    eval_dataset = tokenized[\"test\"],\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"Mymic\")\n",
    "tokenizer.save_pretrained(\"Mymic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model = model, tokenizer = tokenizer)\n",
    "prompt = \"Friend:\\nyou wanna hang?\\nYou:\"\n",
    "output = generator(prompt, max_length = 50, do_sample = True, temperature = 0.6, top_p = 0.9)\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_two_line_response(generated_text):\n",
    "    # Extract everything after \"You:\"\n",
    "    match = re.search(r\"You:\\s*(.*)\", generated_text, re.DOTALL)\n",
    "    response = match.group(1) if match else generated_text\n",
    "\n",
    "    # Split into non-empty lines\n",
    "    lines = [line.strip() for line in response.strip().splitlines() if line.strip()]\n",
    "\n",
    "    # Return the first two lines (joined by newline)\n",
    "    return \"\\n\".join(lines[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Friend:\\nwanna hang?\\nYou:\"\n",
    "\n",
    "raw_output = generator(prompt, max_length=60, do_sample=True, temperature=0.8)[0][\"generated_text\"]\n",
    "cleaned = clean_two_line_response(raw_output)\n",
    "\n",
    "print(\"Mymic:\", cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"Mymic\", 'zip', \"/content/Mymic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"Mymic.zip\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
